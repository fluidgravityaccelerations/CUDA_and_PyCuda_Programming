{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD4jH3ZotYpS"
      },
      "source": [
        "# PyCUDA installation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJDdNH1CBvz0",
        "outputId": "e9f538bd-e81d-47f5-c767-81990a8a73e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2025.1.2.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.2.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pycuda) (4.5.0)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.12/dist-packages (from pycuda) (1.3.10)\n",
            "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
            "  Downloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from pytools>=2011.2->pycuda) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from mako->pycuda) (3.0.3)\n",
            "Downloading pytools-2025.2.5-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2025.1.2-cp312-cp312-linux_x86_64.whl size=659050 sha256=16164a743c2ec3994f3377bf47cb890ea0f33027cf32b54f0e66e5cb70608d06\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/36/f3/ac5f09d768cad3fa15d5a3449bdfe65c3de58e69d036c73228\n",
            "Successfully built pycuda\n",
            "Installing collected packages: siphash24, pytools, pycuda\n",
            "Successfully installed pycuda-2025.1.2 pytools-2025.2.5 siphash24-1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpqdkWPZDXo7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvlvVH34tnWL"
      },
      "source": [
        "# Version #1: using ```SourceModule```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4RYnD8i89Op"
      },
      "source": [
        "PyCUDA initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xYQK81g81vx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- PyCUDA initialization\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv9xrA8_9VUa"
      },
      "source": [
        "\n",
        "iDivUp function: if ```b``` divides ```a```, then ```a/b``` is returned, otherwise the function returns the integer division between ```a``` and ```b``` ```+1```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB-1igHCBM9l"
      },
      "source": [
        "###################\n",
        "# iDivUp FUNCTION #\n",
        "###################\n",
        "def iDivUp(a, b): return (a + b - 1) // b"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP7b2khB-Jha"
      },
      "source": [
        "########\n",
        "# MAIN #\n",
        "########"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjO-Y74f-nJC"
      },
      "source": [
        "Defining two CUDA events that will be used to measure execution time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcTqVeoT-MdR"
      },
      "source": [
        "start = cuda.Event()\n",
        "end   = cuda.Event()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3xgfClb_EQa"
      },
      "source": [
        "Number of array elements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bptd0phi-6vi"
      },
      "source": [
        "N = 100000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT1fEQDh_Ytq"
      },
      "source": [
        "Number of threads per block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-RWRjai_DGi"
      },
      "source": [
        "BLOCKSIZE = 256"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB8BIZ_i_mRC"
      },
      "source": [
        "Create two host vectors ```h_a``` and ```h_b``` of ```N``` random entries. ```np.random.randn``` returns ```float64```'s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JX1Sj20_LlB"
      },
      "source": [
        "h_a = np.random.randn(1, N)\n",
        "h_b = np.random.randn(1, N)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSOaEH68DFq2"
      },
      "source": [
        "Cast ```h_a``` and ```h_b``` to single precision (```float32```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8mZd8jV_k_C"
      },
      "source": [
        "h_a = h_a.astype(np.float32)\n",
        "h_b = h_b.astype(np.float32)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5st--rgFDqB8"
      },
      "source": [
        "Allocate ```h_a.nbytes```, ```h_b.nbytes``` and ```h_c.nbytes``` of GPU device memory space pointed to by ```d_a```, ```d_b``` and ```d_c```, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICyUxA9CA3Xb"
      },
      "source": [
        "d_a = cuda.mem_alloc(h_a.nbytes)\n",
        "d_b = cuda.mem_alloc(h_b.nbytes)\n",
        "d_c = cuda.mem_alloc(h_a.nbytes)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_20z0qpErTs"
      },
      "source": [
        "Copy the ```h_a``` and ```h_b``` arrays from host to the device arrays ```d_a``` and ```d_b```, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlJJ2aYHD7Ys"
      },
      "source": [
        "cuda.memcpy_htod(d_a, h_a)\n",
        "cuda.memcpy_htod(d_b, h_b)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b_lsPJTFKE1"
      },
      "source": [
        "Define the CUDA kernel function ```deviceAdd``` as a string. ```deviceAdd``` performs the elementwise summation of ```d_a``` and ```d_b``` and puts the result in ```d_c```.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0JSS1BvEo2G"
      },
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "  #include <stdio.h>\n",
        "  __global__ void deviceAdd(float * __restrict__ d_c, const float * __restrict__ d_a, const float * __restrict__ d_b, const int N)\n",
        "  {\n",
        "    const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid >= N) return;\n",
        "    d_c[tid] = d_a[tid] + d_b[tid];\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ8He90xF2Ul"
      },
      "source": [
        "Define a reference to the ```__global__``` function ```deviceAdd```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsPvYXgrE48k"
      },
      "source": [
        "deviceAdd = mod.get_function(\"deviceAdd\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXnW0kx9GXbN"
      },
      "source": [
        "Define the block size ```blockDim``` and the grid size ```gridDim```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gJ4KvepF1cw"
      },
      "source": [
        "blockDim  = (BLOCKSIZE, 1, 1)\n",
        "gridDim   = (int(iDivUp(N, BLOCKSIZE)), 1, 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKvUKMC_IIL_"
      },
      "source": [
        "Invoke the ```deviceAdd``` function.\n",
        "Note that, up to here, ```N``` is an *object* of ```class int``` and not an integer number. Therefore, before using it, we must cast it to ```np.int32``` which is essentially the standard, ```32```-bit integer type (matching CUDA’s ```int```).\n",
        "Before launching ```deviceAdd```, the ```start``` and ```end``` events are recorded, so that the execution time can be measured.\n",
        "Note that, before the processing time can be measured, all the activities in the current context must be ceased. This is the reason why ```end.synchronize()``` is invoked. Remember that the host and device executions are asynchronous. Without ```end.synchronize()```, the timing would reflect only the kernel launch latency, not its execution. Furthermore, with the event record, the device will record a time stamp for the event when it reaches that event in the stream. Without synchronization, it happens that the ```end``` event is recorded after the ```deviceAdd``` function execution is actually terminated, as we expect, but the ```print``` function is executed before ```deviceAdd``` has actually finished its execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddx_KUikGTPH",
        "outputId": "6bb3e404-1502-419a-b375-d5389c145879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# --- Warmup execution\n",
        "deviceAdd(d_c, d_a, d_b, np.int32(N), block = blockDim, grid = gridDim)\n",
        "\n",
        "start.record()\n",
        "deviceAdd(d_c, d_a, d_b, np.int32(N), block = blockDim, grid = gridDim)\n",
        "end.record()\n",
        "end.synchronize()\n",
        "secs = start.time_till(end) * 1e-3\n",
        "print(\"Processing time = %fs\" % (secs))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing time = 0.000268s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tjCl9RrSHvK"
      },
      "source": [
        "Allocate host space and copy results from device to host."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wySMagLqGumX"
      },
      "source": [
        "h_c = np.empty_like(h_a)\n",
        "cuda.memcpy_dtoh(h_c, d_c)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs-9eC0YSpiS"
      },
      "source": [
        "Check if the device processing results are as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzVWn4GRSF0j",
        "outputId": "6f96a090-e894-41f9-ad5e-23ba26349739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if np.array_equal(h_c, h_a + h_b):\n",
        "  print(\"Test passed!\")\n",
        "else :\n",
        "  print(\"Error!\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQUSGVoSv6i"
      },
      "source": [
        "Finally, flush context printf buffer. Without flushing, no printout may be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Vvif3iSP2p"
      },
      "source": [
        "cuda.Context.synchronize()"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}